name: Continuous Testing & Reporting

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

jobs:
  # Unit Tests Job
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [18.x, 20.x, 22.x]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Run linting
        run: npm run lint

      - name: Run unit tests with coverage
        run: npm run test:coverage

      - name: Upload unit test coverage
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage/coverage-final.json
          flags: unit-tests
          name: Unit Tests Coverage
          fail_ci_if_error: false

      - name: Upload unit test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results-${{ matrix.node-version }}
          path: |
            coverage/
            test-results/

  # Integration Tests Job
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20.x"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Build application
        run: npm run build

      - name: Run integration tests with coverage
        run: npm run test:integration:coverage
        env:
          CI: true

      - name: Upload integration test coverage
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage/coverage-final.json
          flags: integration-tests
          name: Integration Tests Coverage
          fail_ci_if_error: false

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: |
            coverage/
            test-results/

  # E2E Tests Job
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20.x"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps

      - name: Build application
        run: npm run build

      - name: Start application
        run: npm run preview &
        env:
          PORT: 4173

      - name: Wait for application to be ready
        run: |
          timeout 60 bash -c 'until curl -f http://localhost:4173 > /dev/null 2>&1; do sleep 2; done' || true
        shell: bash

      - name: Run E2E tests
        run: npx playwright test --grep "basic page load"
        env:
          BASE_URL: http://localhost:4173
        continue-on-error: true

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: |
            test-results/
            playwright-report/

      - name: Upload E2E test videos
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: e2e-test-videos
          path: test-results/videos/

  # Performance Tests Job
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20.x"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Build application
        run: npm run build

      - name: Start application for performance testing
        run: npm run preview &
        env:
          PORT: 4173

      - name: Wait for application
        run: |
          timeout 30 bash -c 'until curl -f http://localhost:4173 > /dev/null 2>&1; do sleep 2; done'

      - name: Run Lighthouse performance audit
        run: |
          npx lighthouse http://localhost:4173 \
            --output=json \
            --output-path=./lighthouse-results.json \
            --chrome-flags="--headless --no-sandbox --disable-gpu" \
            --only-categories=performance,accessibility,best-practices,seo

      - name: Upload Lighthouse results
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-results
          path: lighthouse-results.json

  # Test Summary & Reporting Job
  test-summary:
    name: Test Summary & Reporting
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          path: ./artifacts

      - name: Generate test summary report
        run: |
          echo "# 🧪 Continuous Testing Report" > test-summary.md
          echo "" >> test-summary.md
          echo "## 📊 Test Results Summary" >> test-summary.md
          echo "" >> test-summary.md

          # Unit Tests Summary
          echo "### Unit Tests" >> test-summary.md
          if [ -d "artifacts/unit-test-results-18.x" ] || [ -d "artifacts/unit-test-results-20.x" ] || [ -d "artifacts/unit-test-results-22.x" ]; then
            echo "- ✅ Unit tests completed" >> test-summary.md
            echo "- 📈 Coverage reports generated" >> test-summary.md
          else
            echo "- ❌ Unit tests failed or not found" >> test-summary.md
          fi
          echo "" >> test-summary.md

          # Integration Tests Summary
          echo "### Integration Tests" >> test-summary.md
          if [ -d "artifacts/integration-test-results" ]; then
            echo "- ✅ Integration tests completed" >> test-summary.md
            echo "- 🔗 API workflows validated" >> test-summary.md
          else
            echo "- ❌ Integration tests failed or not found" >> test-summary.md
          fi
          echo "" >> test-summary.md

          # E2E Tests Summary
          echo "### E2E Tests" >> test-summary.md
          if [ -d "artifacts/e2e-test-results" ]; then
            echo "- ✅ E2E tests completed" >> test-summary.md
            echo "- 🌐 User journeys validated" >> test-summary.md
          else
            echo "- ❌ E2E tests failed or not found" >> test-summary.md
          fi
          echo "" >> test-summary.md

          # Security Tests Summary
          echo "### Security Tests" >> test-summary.md
          echo "- 🔒 Security audit completed" >> test-summary.md
          echo "- 🛡️ Vulnerability scanning finished" >> test-summary.md
          echo "" >> test-summary.md

          # Performance Tests Summary
          echo "### Performance Tests" >> test-summary.md
          if [ -f "artifacts/lighthouse-results/lighthouse-results.json" ]; then
            echo "- ⚡ Performance audit completed" >> test-summary.md
            echo "- 📊 Lighthouse scores generated" >> test-summary.md
          else
            echo "- ❌ Performance tests failed or not found" >> test-summary.md
          fi
          echo "" >> test-summary.md

          # Overall Status
          echo "## 🎯 Overall Status" >> test-summary.md
          echo "" >> test-summary.md

          UNIT_STATUS="❌"
          INTEGRATION_STATUS="❌"
          E2E_STATUS="❌"
          PERFORMANCE_STATUS="❌"

          [ -d "artifacts/unit-test-results-18.x" ] || [ -d "artifacts/unit-test-results-20.x" ] || [ -d "artifacts/unit-test-results-22.x" ] && UNIT_STATUS="✅"
          [ -d "artifacts/integration-test-results" ] && INTEGRATION_STATUS="✅"
          [ -d "artifacts/e2e-test-results" ] && E2E_STATUS="✅"
          [ -f "artifacts/lighthouse-results/lighthouse-results.json" ] && PERFORMANCE_STATUS="✅"

          echo "| Test Type | Status |" >> test-summary.md
          echo "|-----------|--------|" >> test-summary.md
          echo "| Unit Tests | $UNIT_STATUS |" >> test-summary.md
          echo "| Integration Tests | $INTEGRATION_STATUS |" >> test-summary.md
          echo "| E2E Tests | $E2E_STATUS |" >> test-summary.md
          echo "| Performance Tests | $PERFORMANCE_STATUS |" >> test-summary.md
          echo "" >> test-summary.md

          # Recommendations
          echo "## 💡 Recommendations" >> test-summary.md
          echo "" >> test-summary.md
          echo "- Review test failures and fix any issues" >> test-summary.md
          echo "- Check coverage reports for areas needing more tests" >> test-summary.md
          echo "- Address any security vulnerabilities found" >> test-summary.md
          echo "- Optimize performance based on Lighthouse scores" >> test-summary.md
          echo "- Ensure all artifacts are properly generated" >> test-summary.md

      - name: Upload test summary
        uses: actions/upload-artifact@v4
        with:
          name: test-summary-report
          path: test-summary.md

      - name: Comment PR with test summary
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('test-summary.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

  # Quality Gate Job
  quality-gate:
    name: Quality Gate
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check test results
        run: |
          # Check if all required tests passed
          if [ "${{ needs.unit-tests.result }}" != "success" ]; then
            echo "❌ Unit tests failed"
            exit 1
          fi

          if [ "${{ needs.integration-tests.result }}" != "success" ]; then
            echo "❌ Integration tests failed"
            exit 1
          fi

          if [ "${{ needs.e2e-tests.result }}" != "success" ]; then
            echo "❌ E2E tests failed"
            exit 1
          fi

          echo "✅ All quality gates passed!"

      - name: Create quality gate badge
        run: |
          echo "✅ Quality Gate Passed" > quality-gate-status.txt

      - name: Upload quality gate status
        uses: actions/upload-artifact@v4
        with:
          name: quality-gate-status
          path: quality-gate-status.txt
